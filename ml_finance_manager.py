# -*- coding: utf-8 -*-
"""ML_Finance_manager.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OzDKZxM6bkNa8uiLcAFeA8B2uhcBnUvx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import warnings

warnings.filterwarnings("ignore")

def choose_best_clustering_model(data):
    models = {
        'KMeans': KMeans(),
        'DBSCAN': DBSCAN(),
        'AgglomerativeClustering': AgglomerativeClustering()
    }

    param_grids = {
        'KMeans': {'n_clusters': [2, 3, 4, 5, 6, 7, 8, 9, 10]},
        'DBSCAN': {'eps': [0.3, 0.5, 0.7, 1.0], 'min_samples': [3, 5, 10]},
        'AgglomerativeClustering': {'n_clusters': [2, 3, 4, 5, 6, 7, 8, 9, 10]}
    }

    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    best_model = None
    best_score = -1

    for model_name, model in models.items():
        param_grid = param_grids[model_name]

        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=outer_cv, scoring='accuracy')

        outer_results = []
        for train_idx, test_idx in outer_cv.split(data, np.zeros(data.shape[0])):
            X_train, X_test = data[train_idx], data[test_idx]

            inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=inner_cv, scoring='accuracy')
            grid_search.fit(X_train, np.zeros(X_train.shape[0]))

            best_model = grid_search.best_estimator_
            labels = best_model.fit_predict(X_test)

            if len(set(labels)) > 1:
                test_score = silhouette_score(X_test, labels)
                outer_results.append(test_score)

        if outer_results:
            average_performance = np.mean(outer_results)
            print(f"{model_name} Average Performance: {average_performance}")

            if average_performance > best_score:
                best_score = average_performance
                best_model = grid_search.best_estimator_

    return best_model

file_path = '/content/trans.xlsx'
data = pd.read_excel(file_path)

# Extract the required columns
descriptions = data.iloc[:, 2]
debits = data.iloc[:, 4]

def extract_transaction_name(description):
    parts = description.split('/')
    return parts[-2] if len(parts) > 1 else ''

data['TransactionName'] = descriptions.apply(lambda x: extract_transaction_name(x) if isinstance(x, str) else '')

# Convert non-numeric debits to NaN and drop them
debits = pd.to_numeric(debits, errors='coerce')
data = data[debits.notna()]
data['DebitedAmount'] = debits.dropna()

# Normalize the data
scaler = StandardScaler()
debits_normalized = scaler.fit_transform(data['DebitedAmount'].values.reshape(-1, 1))

unique_values = np.unique(debits_normalized)
if len(unique_values) < 2:
    print("Not enough variability in data for clustering.")
    categories = np.zeros(debits_normalized.shape[0], dtype=int)
else:
    best_model = choose_best_clustering_model(debits_normalized)
    categories = best_model.fit_predict(debits_normalized)

category_mapping = {i: category for i, category in enumerate(['Low', 'Medium', 'High'])}
data['Category'] = [category_mapping[label] for label in categories]

# Plot the spending categories in a pie chart
category_counts = data['Category'].value_counts()
category_counts.plot.pie(autopct='%1.1f%%')
plt.title('Spending Categories')
plt.show()

transaction_frequency = data['TransactionName'].value_counts()

frequency_price_df = pd.DataFrame({
    'Frequency': data['TransactionName'].map(transaction_frequency),
    'Price': data['DebitedAmount'],
    'Category': data['Category']
})

# Plot frequency vs. price with different colors for each category
categories_unique = frequency_price_df['Category'].unique()
colors = plt.cm.get_cmap('tab10', len(categories_unique))

plt.figure(figsize=(10, 6))
for i, category in enumerate(categories_unique):
    subset = frequency_price_df[frequency_price_df['Category'] == category]
    plt.scatter(subset['Frequency'], subset['Price'], color=colors(i), label=category, alpha=0.6, edgecolors='w', s=100)

plt.xlabel('Frequency')
plt.ylabel('Price')
plt.title('Frequency vs. Price')
plt.legend()
plt.grid(True)
plt.show()

def recommend_savings(expenses, target_savings):
    current_spending = expenses['DebitedAmount'].sum()
    reduction_percentage = target_savings / current_spending
    expenses['RecommendedSavings'] = expenses['DebitedAmount'] * reduction_percentage
    return expenses

unnecessary_expenses = data[data['Category'] == 'Low']
target_savings = float(input("Enter the amount you want to save: "))
savings_recommendations = recommend_savings(unnecessary_expenses, target_savings)
print(savings_recommendations[['TransactionName', 'Category', 'DebitedAmount', 'RecommendedSavings']])

def predict_spending(data, periods=3):
    predictions = {}

    for category in data['Category'].unique():
        category_data = data[data['Category'] == category]
        category_debits = category_data['DebitedAmount']

        if len(category_debits) < 2:
            print(f"Not enough data to forecast for category {category}.")
            continue

        seasonal_periods = min(3, len(category_debits) - 1)
        model = ExponentialSmoothing(category_debits, trend='add', seasonal='add', seasonal_periods=seasonal_periods)
        fitted_model = model.fit()
        forecast = fitted_model.forecast(periods)
        predictions[category] = forecast

    return predictions

spending_predictions = predict_spending(data)
print(spending_predictions)